# üìä LatAm Tech Job Market Intelligence MVP
Pipeline automatizado de Scraping y An√°lisis de Datos en tiempo real para el mercado laboral tecnol√≥gico en Latinoam√©rica.

[![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-Automatizaci√≥n-blue?logo=githubactions)](https://github.com/tu-usuario/tu-repo/actions)
[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-red?logo=streamlit)](https://latam-marketscraper-dashboard.streamlit.app/)
[![Supabase](https://img.shields.io/badge/Supabase-Base_de_Datos-green?logo=supabase)](https://supabase.com)

---

## üìã Tabla de Contenidos
1. [Descripci√≥n del Proyecto](#descripci√≥n-del-proyecto)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Estructura del Proyecto](#estructura-del-proyecto)
4. [Pipeline de Datos e Inteligencia](#pipeline-de-datos-e-inteligencia)
5. [Funcionalidades del Dashboard](#funcionalidades-del-dashboard)
6. [Requisitos Previos](#requisitos-previos)
7. [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
8. [Ejecuci√≥n del Sistema](#ejecuci√≥n-del-sistema)
9. [Despliegue](#despliegue)
10. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
11. [M√©tricas de √âxito](#m√©tricas-de-√©xito)
12. [Pr√≥ximos Pasos](#pr√≥ximos-pasos)

---

## Descripci√≥n del Proyecto

Este MVP proporciona una visi√≥n anal√≠tica del mercado laboral tech en LATAM. No solo recolecta vacantes, sino que analiza la transparencia salarial y la demanda por sectores espec√≠ficos, ayudando a profesionales a tomar decisiones basadas en datos reales.

**Objetivo:** Procesar 500+ registros diarios con foco en M√©xico, Colombia, Argentina, Chile, Per√∫ y Ecuador.

**Propuesta de Valor:** Ayudar a entender la demanda del mercado, roles emergentes y habilidades requeridas en el sector tecnol√≥gico latinoamericano.

---

## Arquitectura del Sistema

```
graph TD
    subgraph Fuentes_Externas [Fuentes de Datos]
        A1[LinkedIn]
        A2[Computrabajo]
        A3[GetonBoard]
    end

    subgraph Orquestacion [Automatizaci√≥n & ETL]
        B1[GitHub Actions]
        B2[Scrapy Spiders]
        B3[Pipeline de Clasificaci√≥n]
        B4[Script update_data.py]
    end

    subgraph Almacenamiento [Cloud Database]
        C1[(Supabase / PostgreSQL)]
    end

    subgraph Visualizacion [Frontend]
        D1[Streamlit Dashboard]
        D2[Plotly Interactive Charts]
    end

    %% Flujos
    A1 & A2 & A3 --> B2
    B1 -->|Trigger Diario| B2
    B2 -->|Item Raw| B3
    B3 -->|Item Categorizado| C1
    C1 -->|Fetch Data| B4
    B4 -->|Clean & Update| C1
    C1 -.->|Real-time Sync| D1
    D1 --> D2
```

---

## Estructura del Proyecto

```
latam-tech-job-market-intelligence/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ scrape_daily.yml          # Programador GitHub Actions
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scrapy.cfg
‚îÇ   ‚îî‚îÄ‚îÄ jobscraper/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ settings.py               # Configuraci√≥n de Scrapy
‚îÇ       ‚îú‚îÄ‚îÄ items.py                  # Modelos de datos
‚îÇ       ‚îú‚îÄ‚îÄ pipelines.py              # ETL e inserci√≥n en BD
‚îÇ       ‚îî‚îÄ‚îÄ spiders/
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ getonboard_spider.py  # Scraper GetonBoard
‚îÇ           ‚îú‚îÄ‚îÄ computrabajo_spider.py # Scraper Computrabajo
‚îÇ           ‚îî‚îÄ‚îÄ linkedin_spider.py    # Scraper LinkedIn
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cleaning.py                   # Funciones de limpieza
‚îÇ   ‚îî‚îÄ‚îÄ update_data.py                # Estandarizaci√≥n
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql                    # Esquema de base de datos
‚îÇ   ‚îî‚îÄ‚îÄ queries.py                    # Consultas frecuentes
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                   # Configuraci√≥n general
‚îú‚îÄ‚îÄ app.py                            # App principal de Streamlit
‚îú‚îÄ‚îÄ .env.example                      # Plantilla de variables de entorno
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt                  # Librer√≠as para ejecutar la interfaz de usuario
‚îú‚îÄ‚îÄ requirements_scraper.txt          # Dependencias para la extracci√≥n y procesamiento de datos.
‚îî‚îÄ‚îÄ test_conection.py                 # Script para testear conexi√≥n a Supabase
```

---

## Pipeline de Datos e Inteligencia

El sistema procesa cada vacante a trav√©s de un pipeline de limpieza y clasificaci√≥n de 4 etapas:

- **Limpieza (ETL):** Normalizaci√≥n de salarios, eliminaci√≥n de HTML y manejo de duplicados.
- **Clasificaci√≥n por Sector:** Motor de reglas basado en keywords para categorizar vacantes en *Fintech, EdTech, AI & Machine Learning*, entre otros.
- **Extracci√≥n de Skills:** Identificaci√≥n autom√°tica de tecnolog√≠as y habilidades requeridas por vacante.
- **Auditor√≠a de Calidad:** C√°lculo de un *Data Quality Score* basado en la completitud de la informaci√≥n (descripci√≥n, salario, requisitos).

---

## Funcionalidades del Dashboard

- **Distribuci√≥n por Sector:** Identificaci√≥n inteligente de industrias (Fintech, EdTech, IA, E-commerce, HealthTech, Cibersecurity).
- **Quality Score:** An√°lisis de completitud de datos por plataforma.
- **Geolocalizaci√≥n:** Mapa de calor de vacantes por pa√≠s en LATAM.

---

## Requisitos Previos

### Cuentas necesarias (¬°todas gratuitas!)
1. **GitHub** ‚Äî Para alojar el c√≥digo y automatizar la ejecuci√≥n.
2. **Supabase** ‚Äî Base de datos PostgreSQL (500MB en plan gratuito).
3. **Streamlit Cloud** ‚Äî Hosting del dashboard.

### Requisitos locales
- Python 3.9 o superior
- Git
- Editor de c√≥digo (VS Code recomendado)
- Acceso a terminal

---

## Instalaci√≥n y Configuraci√≥n

### Fase 1: Configuraci√≥n del Entorno Local

#### 1. Clonar y configurar el proyecto
```bash
# Crear directorio del proyecto
mkdir latam-tech-job-market-intelligence
cd latam-tech-job-market-intelligence

# Inicializar git
git init

# Crear entorno virtual
python -m .venv venv

# Activar entorno virtual
# En Windows:
.venv\Scripts\activate
# En Mac/Linux:
source .venv/bin/activate

# Crear estructura de carpetas
mkdir -p scrapers/jobscraper/spiders
mkdir -p etl database config 
mkdir -p .github/workflows
```

#### 2. Instalar dependencias
El proyecto utiliza una estructura de requerimientos dividida para optimizar el despliegue en diferentes entornos (Scrapers en GitHub Actions y Dashboard en Streamlit Cloud).

1. requirements.txt (Entorno del Dashboard)
Este archivo contiene las librer√≠as necesarias para ejecutar la interfaz de usuario y la visualizaci√≥n de datos. Es el que utiliza Streamlit Cloud para desplegar la aplicaci√≥n.

Librer√≠as clave: streamlit, plotly, pandas, supabase.

Uso: 
```bash
pip install -r requirements.txt
```

2. requirements_scraper.txt (Entorno del Pipeline ETL)
Contiene las dependencias cr√≠ticas para la extracci√≥n y procesamiento de datos. Est√° dise√±ado para ser ligero y evitar conflictos de versiones durante la automatizaci√≥n en GitHub Actions.

Librer√≠as clave: scrapy, supabase==2.11.0, gotrue==2.11.0, python-dotenv.

Nota t√©cnica: Se han fijado versiones espec√≠ficas de supabase y gotrue para garantizar la compatibilidad con entornos de servidor y evitar errores de handshake/proxy.

Uso: 
```bash
pip install -r requirements_scraper.txt
```


#### 3. Configurar variables de entorno
Crear archivo `.env`:
```env
SUPABASE_URL=tu_supabase_url_aqui
SUPABASE_SERVICE_KEY=tu_supabase_service_role_key_aqui
```

Crear `.env.example` (para el equipo):
```env
SUPABASE_URL=
SUPABASE_SERVICE_KEY=
```

#### 4. Crear `.gitignore`
```
# Python
__pycache__/
*.py[cod]
*$py.class
venv/
*.egg-info/

# Variables de entorno
.env

# Scrapy
.scrapy/
httpcache/

# IDE
.vscode/
.idea/

# Logs
*.log
scraper.log

# Datos
*.csv
*.json
data/
```

---

### Fase 2: Configuraci√≥n de la Base de Datos

#### 1. Crear proyecto en Supabase
1. Ir a [https://supabase.com](https://supabase.com) e iniciar sesi√≥n con GitHub.
2. Hacer clic en **New Project** y completar:
   - **Name**: latam-tech-job-market-intelligence
   - **Database Password**: (guardar en lugar seguro)
   - **Region**: US East (m√°s cercana a LATAM)
3. Esperar 2-3 minutos hasta que el proyecto est√© listo.

#### 2. Ejecutar el esquema de base de datos
1. En el dashboard de Supabase, ir a **SQL Editor**.
2. Copiar el contenido de `database/schema.sql` y ejecutarlo.
3. Verificar que las tablas se hayan creado en **Table Editor**.

#### 3. Obtener credenciales
1. Ir a **Project Settings** ‚Üí **API**.
2. Copiar:
   - **Project URL** (ej: `https://xxxxx.supabase.co`)
   - **anon public key** (cadena larga que empieza con `eyJ...`)
3. Agregar ambos valores al archivo `.env`.

#### 4. Probar la conexi√≥n
Crear `test_connection.py`:
```python
from supabase import create_client
import os
from dotenv import load_dotenv

load_dotenv()

url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_SERVICE_KEY')

client = create_client(url, key)
response = client.table('jobs').select('*').limit(1).execute()

print("‚úÖ Conexi√≥n exitosa!" if response else "‚ùå Conexi√≥n fallida")
```

Ejecutar:
```bash
python test_conection.py
```

---

### Fase 3: Configuraci√≥n de Scrapy

#### 1. Inicializar el proyecto Scrapy
```bash
cd scrapers
scrapy startproject jobscraper
cd jobscraper
```

#### 2. Agregar los archivos del proyecto
Asegurarse de que los siguientes archivos est√©n en su lugar:
- `items.py` ‚Äî Modelos de datos
- `pipelines.py` ‚Äî L√≥gica ETL
- `settings.py` ‚Äî Configuraci√≥n de Scrapy
- `spiders/getonboard_spider.py`
- `spiders/computrabajo_spider.py`
- `spiders/linkedin_spider.py`

#### 3. Probar un spider
```bash
# Probar spider de GetonBoard
scrapy crawl getonboard -o test_output.json

# Verificar resultado
cat test_output.json
```

Resultado esperado: array JSON con datos de vacantes.

---

### Fase 4: Configuraci√≥n del Dashboard

#### 1. Probar el dashboard localmente
```bash
streamlit run app.py
```

Se abrir√° el navegador en `http://localhost:8501`.

> **Nota:** El dashboard puede mostrar "sin datos" si los scrapers a√∫n no han corrido.

---

### Fase 5: Configuraci√≥n de la Automatizaci√≥n

#### 1. Verificar el workflow de GitHub Actions
El archivo `.github/workflows/scrape_daily.yml` ya contiene la programaci√≥n diaria a las 6:00 AM UTC.

#### 2. Probar la ejecuci√≥n localmente
```bash
python run_scraper.py
```

Esto ejecutar√° los tres scrapers de forma secuencial.

---

## Ejecuci√≥n del Sistema

### Desarrollo Local

#### Ejecutar un spider individual
```bash
cd scrapers/jobscraper
scrapy crawl getonboard
```

#### Ejecutar todos los scrapers
```bash
python run_scraper.py
```



#### Iniciar el dashboard
```bash
streamlit run app.py
```

---

## Despliegue

### 1. Subir el proyecto a GitHub

```bash
git add .
git commit -m "Initial commit: LatAm Job Intelligence MVP"
git remote add origin https://github.com/TU-USUARIO/latam-job-intelligence.git
git push -u origin main
```

### 2. Configurar secretos en GitHub

1. Ir al repositorio ‚Üí **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions**.
2. Agregar los siguientes secretos:
   - `SUPABASE_URL`
   - `SUPABASE_SERVICE_KEY`

### 3. Activar GitHub Actions

1. Ir a la pesta√±a **Actions** del repositorio.
2. Verificar que el workflow **Daily Job Scraping** est√© visible.
3. Hacer clic en **Enable workflow**.
4. Opcionalmente, ejecutar manualmente con **Run workflow** para una prueba inmediata.

> **Programaci√≥n:** Se ejecuta autom√°ticamente todos los d√≠as a las 6:00 AM UTC (1-3 AM hora LATAM).

### 4. Desplegar el Dashboard en Streamlit Cloud

1. Ir a [https://streamlit.io/cloud](https://streamlit.io/cloud) e iniciar sesi√≥n con GitHub.
2. Hacer clic en **New app** y seleccionar:
   - **Repository**: tu-usuario/latam-job-intelligence
   - **Branch**: main
   - **Main file**: app.py
3. En **Advanced settings**, agregar los secretos `SUPABASE_URL` y `SUPABASE_KEY`.
4. Hacer clic en **Deploy**.

**Resultado:** El dashboard est√° disponible en [https://latam-marketscraper-dashboard.streamlit.app](https://latam-marketscraper-dashboard.streamlit.app/).

---


## Soluci√≥n de Problemas

### Scrapers con errores
```bash
# Revisar logs
cat scraper.log

# Probar spider en modo detallado
scrapy crawl getonboard -L DEBUG

# Inspeccionar la estructura del sitio
scrapy shell "https://www.getonbrd.com/jobs"
```

**Soluci√≥n:** Actualizar los selectores CSS en el c√≥digo del spider si la estructura del sitio cambi√≥.

---

### Error de conexi√≥n a la base de datos
```bash
# Probar conexi√≥n
python test_connection.py

# Verificar credenciales
cat .env
```

**Soluci√≥n:** Confirmar que la URL y la clave de Supabase sean correctas.

---

### GitHub Actions con errores
1. Ir a la pesta√±a **Actions** y hacer clic en el workflow fallido.
2. Revisar los logs del error.
3. Causas m√°s comunes:
   - Los secretos no est√°n configurados correctamente.
   - El `requirements.txt` est√° incompleto.
   - La estructura del proyecto Scrapy tiene inconsistencias.

---

### Dashboard sin datos
```bash
# Verificar datos en Supabase
# Ir a Supabase ‚Üí Table Editor ‚Üí tabla jobs

# Probar localmente
streamlit run app.py
```

**Soluci√≥n:** Ejecutar los scrapers primero si la tabla a√∫n no tiene datos.

---

## M√©tricas de √âxito

| M√©trica | Objetivo |
|---|---|
| Vacantes procesadas | 500+ por semana |
| Pa√≠ses cubiertos | 3+ (MX, CO, AR, CL, PE, EC) |
| Sectores clasificados | 5+ (EdTech, Fintech, IA, E-commerce, HealthTech) |
| Skills extra√≠das | 50+ |
| Uptime GitHub Actions | 95%+ |
| Frecuencia de actualizaci√≥n | Diaria |
|

---

## Pr√≥ximos Pasos

1. Agregar notificaciones por email con reportes diarios autom√°ticos.
2. Implementar modelo de ML para predicci√≥n de salarios.
3. Integrar fuentes adicionales de empleo (alternativas a LinkedIn API).
4. Crear dashboard responsive para m√≥viles.
5. Agregar autenticaci√≥n de usuarios para insights personalizados.
6. Implementar algoritmo de matching entre perfiles y vacantes.

---

## üìû Soporte

- **Issues:** Crear un issue en el repositorio de GitHub.
- **Preguntas:** Canal de Slack del equipo.
- **Documentaci√≥n adicional:** Carpeta `docs/` del repositorio.

---

## üìÑ Licencia

Este proyecto fue desarrollado con fines educativos como parte de una simulaci√≥n laboral en tecnolog√≠a.

---

**Construido con ‚ù§Ô∏è para el Talento Tech de LATAM**

*√öltima actualizaci√≥n: Febrero 2026*