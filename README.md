# üìä LatAm Tech Market Intelligence MVP
Pipeline automatizado de Scraping y An√°lisis de Datos en tiempo real para el mercado laboral tecnol√≥gico en Latinoam√©rica.

[![GitHub Actions](https://img.shields.io/badge/GitHub_Actions-Automatizaci√≥n-blue?logo=githubactions)](https://github.com/tu-usuario/tu-repo/actions)
[![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-red?logo=streamlit)](https://latam-marketscraper-dashboard.streamlit.app/)
[![Supabase](https://img.shields.io/badge/Supabase-Base_de_Datos-green?logo=supabase)](https://supabase.com)

---

## üìã Tabla de Contenidos
1. [Descripci√≥n del Proyecto](#descripci√≥n-del-proyecto)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Estructura del Proyecto](#estructura-del-proyecto)
4. [Pipeline de Datos e Inteligencia](#pipeline-de-datos-e-inteligencia)
5. [Funcionalidades del Dashboard](#funcionalidades-del-dashboard)
6. [Requisitos Previos](#requisitos-previos)
7. [Instalaci√≥n y Configuraci√≥n](#instalaci√≥n-y-configuraci√≥n)
8. [Ejecuci√≥n del Sistema](#ejecuci√≥n-del-sistema)
9. [Despliegue](#despliegue)
10. [Flujo de Trabajo del Equipo](#flujo-de-trabajo-del-equipo)
11. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)
12. [M√©tricas de √âxito](#m√©tricas-de-√©xito)
13. [Recursos de Aprendizaje](#recursos-de-aprendizaje)
14. [Pr√≥ximos Pasos](#pr√≥ximos-pasos)

---

## Descripci√≥n del Proyecto

Este MVP proporciona una visi√≥n anal√≠tica del mercado laboral tech en LATAM. No solo recolecta vacantes, sino que analiza la transparencia salarial y la demanda por sectores espec√≠ficos, ayudando a profesionales a tomar decisiones basadas en datos reales.

**Objetivo:** Procesar 500+ registros diarios con foco en M√©xico, Colombia, Argentina, Chile, Per√∫ y Ecuador.

**Propuesta de Valor:** Ayudar a entender la demanda del mercado, roles emergentes y habilidades requeridas en el sector tecnol√≥gico latinoamericano.

---

## Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     GitHub Actions (Diario 6 AM UTC)        ‚îÇ
‚îÇ          Automatizaci√≥n Programada          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Scrapy Spiders (3 fuentes)          ‚îÇ
‚îÇ  ‚Ä¢ GetonBoard  ‚Ä¢ Torre API  ‚Ä¢ Computrabajo  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Pipeline ETL (4 etapas)            ‚îÇ
‚îÇ  Limpiar ‚Üí Extraer Skills ‚Üí Clasificar ‚Üí Guardar ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Supabase (PostgreSQL)                ‚îÇ
‚îÇ   Tablas: jobs, companies, skills, trends   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Streamlit Dashboard (Tiempo Real)        ‚îÇ
‚îÇ  Visualizaciones, Reportes, Exportaci√≥n     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Estructura del Proyecto

```
latam-job-intelligence/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ scrape_daily.yml          # Programador GitHub Actions
‚îú‚îÄ‚îÄ scrapers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ scrapy.cfg
‚îÇ   ‚îî‚îÄ‚îÄ jobscraper/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ settings.py               # Configuraci√≥n de Scrapy
‚îÇ       ‚îú‚îÄ‚îÄ items.py                  # Modelos de datos
‚îÇ       ‚îú‚îÄ‚îÄ pipelines.py              # ETL e inserci√≥n en BD
‚îÇ       ‚îî‚îÄ‚îÄ spiders/
‚îÇ           ‚îú‚îÄ‚îÄ __init__.py
‚îÇ           ‚îú‚îÄ‚îÄ getonboard_spider.py  # Scraper GetonBoard
‚îÇ           ‚îú‚îÄ‚îÄ computrabajo_spider.py # Scraper Computrabajo
‚îÇ           ‚îî‚îÄ‚îÄ linkedin_spider.py    # Scraper LinkedIn
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cleaning.py                   # Funciones de limpieza
‚îÇ   ‚îî‚îÄ‚îÄ update_data.py                # Estandarizaci√≥n
‚îú‚îÄ‚îÄ database/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ schema.sql                    # Esquema de base de datos
‚îÇ   ‚îî‚îÄ‚îÄ queries.py                    # Consultas frecuentes
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                   # Configuraci√≥n general
‚îú‚îÄ‚îÄ app.py                            # App principal de Streamlit
‚îú‚îÄ‚îÄ .env.example                      # Plantilla de variables de entorno
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ requirements_scraper.txt
```

---

## Pipeline de Datos e Inteligencia

El sistema procesa cada vacante a trav√©s de un pipeline de limpieza y clasificaci√≥n de 4 etapas:

- **Limpieza (ETL):** Normalizaci√≥n de salarios, eliminaci√≥n de HTML y manejo de duplicados.
- **Clasificaci√≥n por Sector:** Motor de reglas basado en keywords para categorizar vacantes en *Fintech, EdTech, AI & Machine Learning*, entre otros.
- **Extracci√≥n de Skills:** Identificaci√≥n autom√°tica de tecnolog√≠as y habilidades requeridas por vacante.
- **Auditor√≠a de Calidad:** C√°lculo de un *Data Quality Score* basado en la completitud de la informaci√≥n (descripci√≥n, salario, requisitos).

---

## Funcionalidades del Dashboard

- **Distribuci√≥n por Sector:** Identificaci√≥n inteligente de industrias (Fintech, EdTech, IA).
- **Quality Score:** An√°lisis de completitud de datos por plataforma.
- **Geolocalizaci√≥n:** Mapa de calor de vacantes por pa√≠s en LATAM.

---

## Requisitos Previos

### Cuentas necesarias (¬°todas gratuitas!)
1. **GitHub** ‚Äî Para alojar el c√≥digo y automatizar la ejecuci√≥n.
2. **Supabase** ‚Äî Base de datos PostgreSQL (500MB en plan gratuito).
3. **Streamlit Cloud** ‚Äî Hosting del dashboard.

### Requisitos locales
- Python 3.9 o superior
- Git
- Editor de c√≥digo (VS Code recomendado)
- Acceso a terminal

---

## Instalaci√≥n y Configuraci√≥n

### Fase 1: Configuraci√≥n del Entorno Local

#### 1. Clonar y configurar el proyecto
```bash
# Crear directorio del proyecto
mkdir latam-job-intelligence
cd latam-job-intelligence

# Inicializar git
git init

# Crear entorno virtual
python -m venv venv

# Activar entorno virtual
# En Windows:
venv\Scripts\activate
# En Mac/Linux:
source venv/bin/activate

# Crear estructura de carpetas
mkdir -p scrapers/jobscraper/spiders
mkdir -p etl database analysis dashboard/pages config notebooks tests
mkdir -p .github/workflows
```

#### 2. Instalar dependencias
Crear `requirements.txt`:
```txt
scrapy==2.11.0
beautifulsoup4==4.12.2
selenium==4.15.2
requests==2.31.0
supabase==2.3.0
python-dotenv==1.0.0
pandas==2.1.4
numpy==1.26.2
plotly==5.18.0
streamlit==1.29.0
pyyaml==6.0.1
fake-useragent==1.4.0
python-dateutil==2.8.2
```

Instalar:
```bash
pip install -r requirements.txt
```

#### 3. Configurar variables de entorno
Crear archivo `.env`:
```env
SUPABASE_URL=tu_supabase_url_aqui
SUPABASE_KEY=tu_supabase_anon_key_aqui
```

Crear `.env.example` (para el equipo):
```env
SUPABASE_URL=
SUPABASE_KEY=
```

#### 4. Crear `.gitignore`
```
# Python
__pycache__/
*.py[cod]
*$py.class
venv/
*.egg-info/

# Variables de entorno
.env

# Scrapy
.scrapy/
httpcache/

# IDE
.vscode/
.idea/

# Logs
*.log
scraper.log

# Datos
*.csv
*.json
data/
```

---

### Fase 2: Configuraci√≥n de la Base de Datos

#### 1. Crear proyecto en Supabase
1. Ir a [https://supabase.com](https://supabase.com) e iniciar sesi√≥n con GitHub.
2. Hacer clic en **New Project** y completar:
   - **Name**: latam-job-intelligence
   - **Database Password**: (guardar en lugar seguro)
   - **Region**: US East (m√°s cercana a LATAM)
3. Esperar 2-3 minutos hasta que el proyecto est√© listo.

#### 2. Ejecutar el esquema de base de datos
1. En el dashboard de Supabase, ir a **SQL Editor**.
2. Copiar el contenido de `database/schema.sql` y ejecutarlo.
3. Verificar que las tablas se hayan creado en **Table Editor**.

#### 3. Obtener credenciales
1. Ir a **Project Settings** ‚Üí **API**.
2. Copiar:
   - **Project URL** (ej: `https://xxxxx.supabase.co`)
   - **anon public key** (cadena larga que empieza con `eyJ...`)
3. Agregar ambos valores al archivo `.env`.

#### 4. Probar la conexi√≥n
Crear `test_connection.py`:
```python
from supabase import create_client
import os
from dotenv import load_dotenv

load_dotenv()

url = os.getenv('SUPABASE_URL')
key = os.getenv('SUPABASE_KEY')

client = create_client(url, key)
response = client.table('jobs').select('*').limit(1).execute()

print("‚úÖ Conexi√≥n exitosa!" if response else "‚ùå Conexi√≥n fallida")
```

Ejecutar:
```bash
python test_connection.py
```

---

### Fase 3: Configuraci√≥n de Scrapy

#### 1. Inicializar el proyecto Scrapy
```bash
cd scrapers
scrapy startproject jobscraper
cd jobscraper
```

#### 2. Agregar los archivos del proyecto
Asegurarse de que los siguientes archivos est√©n en su lugar:
- `items.py` ‚Äî Modelos de datos
- `pipelines.py` ‚Äî L√≥gica ETL
- `settings.py` ‚Äî Configuraci√≥n de Scrapy
- `spiders/getonboard_spider.py`
- `spiders/computrabajo_spider.py`
- `spiders/linkedin_spider.py`

#### 3. Probar un spider
```bash
# Probar spider de GetonBoard
scrapy crawl getonboard -o test_output.json

# Verificar resultado
cat test_output.json
```

Resultado esperado: array JSON con datos de vacantes.

---

### Fase 4: Configuraci√≥n del Dashboard

#### 1. Probar el dashboard localmente
```bash
streamlit run app.py
```

Se abrir√° el navegador en `http://localhost:8501`.

> **Nota:** El dashboard puede mostrar "sin datos" si los scrapers a√∫n no han corrido.

---

### Fase 5: Configuraci√≥n de la Automatizaci√≥n

#### 1. Verificar el workflow de GitHub Actions
El archivo `.github/workflows/scrape_daily.yml` ya contiene la programaci√≥n diaria a las 6:00 AM UTC.

#### 2. Probar la ejecuci√≥n localmente
```bash
python run_scraper.py
```

Esto ejecutar√° los tres scrapers de forma secuencial.

---

## Ejecuci√≥n del Sistema

### Desarrollo Local

#### Ejecutar un spider individual
```bash
cd scrapers/jobscraper
scrapy crawl getonboard
```

#### Ejecutar todos los scrapers
```bash
python run_scraper.py
```

#### Generar reporte
```bash
python analysis/report_generator.py
```

#### Iniciar el dashboard
```bash
streamlit run app.py
```

---

## Despliegue

### 1. Subir el proyecto a GitHub

```bash
git add .
git commit -m "Initial commit: LatAm Job Intelligence MVP"
git remote add origin https://github.com/TU-USUARIO/latam-job-intelligence.git
git push -u origin main
```

### 2. Configurar secretos en GitHub

1. Ir al repositorio ‚Üí **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions**.
2. Agregar los siguientes secretos:
   - `SUPABASE_URL`
   - `SUPABASE_KEY`

### 3. Activar GitHub Actions

1. Ir a la pesta√±a **Actions** del repositorio.
2. Verificar que el workflow **Daily Job Scraping** est√© visible.
3. Hacer clic en **Enable workflow**.
4. Opcionalmente, ejecutar manualmente con **Run workflow** para una prueba inmediata.

> **Programaci√≥n:** Se ejecuta autom√°ticamente todos los d√≠as a las 6:00 AM UTC (1-3 AM hora LATAM).

### 4. Desplegar el Dashboard en Streamlit Cloud

1. Ir a [https://streamlit.io/cloud](https://streamlit.io/cloud) e iniciar sesi√≥n con GitHub.
2. Hacer clic en **New app** y seleccionar:
   - **Repository**: tu-usuario/latam-job-intelligence
   - **Branch**: main
   - **Main file**: app.py
3. En **Advanced settings**, agregar los secretos `SUPABASE_URL` y `SUPABASE_KEY`.
4. Hacer clic en **Deploy**.

**Resultado:** El dashboard quedar√° disponible en `https://tu-app.streamlit.app`.

---

## Flujo de Trabajo del Equipo

### Operaciones Diarias

1. **Scraping automatizado:** Se ejecuta diariamente v√≠a GitHub Actions.
2. **Actualizaci√≥n de datos:** Fluye autom√°ticamente hacia Supabase.
3. **Dashboard:** Se actualiza al refrescar el navegador.
4. **Monitoreo:** Revisar logs en la pesta√±a Actions de GitHub.

### Roles del Equipo

#### Desarrollador 1 ‚Äî Spiders y ETL
- Mantener el c√≥digo de los scrapers.
- Corregir scrapers que fallen.
- Agregar nuevas fuentes de datos.
- Mejorar la l√≥gica de extracci√≥n.

#### Desarrollador 2 ‚Äî Datos y An√°lisis
- Monitorear la calidad de los datos.
- Crear notebooks de an√°lisis.
- Generar insights del mercado.
- Actualizar algoritmos de tendencias.

#### Desarrollador 3 ‚Äî Dashboard y Reportes
- Mejorar las visualizaciones.
- Agregar nuevas funcionalidades al dashboard.
- Generar reportes semanales.
- Mejorar la experiencia de usuario.

### Rutina Semanal

**Lunes:** Revisar logs del fin de semana, verificar m√©tricas de calidad y planificar mejoras de la semana.

**Mi√©rcoles:** An√°lisis de datos a mitad de semana, prueba de nuevas funcionalidades en local y actualizaci√≥n de documentaci√≥n.

**Viernes:** Despliegue de mejoras, generaci√≥n del reporte semanal y sesi√≥n de conocimiento compartido del equipo.

---

## Soluci√≥n de Problemas

### Scrapers con errores
```bash
# Revisar logs
cat scraper.log

# Probar spider en modo detallado
scrapy crawl getonboard -L DEBUG

# Inspeccionar la estructura del sitio
scrapy shell "https://www.getonbrd.com/jobs"
```

**Soluci√≥n:** Actualizar los selectores CSS en el c√≥digo del spider si la estructura del sitio cambi√≥.

---

### Error de conexi√≥n a la base de datos
```bash
# Probar conexi√≥n
python test_connection.py

# Verificar credenciales
cat .env
```

**Soluci√≥n:** Confirmar que la URL y la clave de Supabase sean correctas.

---

### GitHub Actions con errores
1. Ir a la pesta√±a **Actions** y hacer clic en el workflow fallido.
2. Revisar los logs del error.
3. Causas m√°s comunes:
   - Los secretos no est√°n configurados correctamente.
   - El `requirements.txt` est√° incompleto.
   - La estructura del proyecto Scrapy tiene inconsistencias.

---

### Dashboard sin datos
```bash
# Verificar datos en Supabase
# Ir a Supabase ‚Üí Table Editor ‚Üí tabla jobs

# Probar localmente
streamlit run app.py
```

**Soluci√≥n:** Ejecutar los scrapers primero si la tabla a√∫n no tiene datos.

---

## M√©tricas de √âxito

| M√©trica | Objetivo |
|---|---|
| Vacantes procesadas | 500+ por semana |
| Pa√≠ses cubiertos | 3+ (MX, CO, AR, CL, PE, EC) |
| Sectores clasificados | 3+ (EdTech, Fintech, IA) |
| Skills extra√≠das | 50+ |
| Uptime GitHub Actions | 95%+ |
| Frecuencia de actualizaci√≥n | Diaria |

---

## Recursos de Aprendizaje

**Scrapy**
- [Documentaci√≥n oficial](https://docs.scrapy.org/)
- [Tutorial introductorio](https://docs.scrapy.org/en/latest/intro/tutorial.html)

**Supabase**
- [Gu√≠a de inicio r√°pido](https://supabase.com/docs/guides/getting-started)
- [Cliente Python](https://supabase.com/docs/reference/python/introduction)

**Streamlit**
- [Primeros pasos](https://docs.streamlit.io/library/get-started)
- [Galer√≠a de ejemplos](https://streamlit.io/gallery)

---

## Pr√≥ximos Pasos

1. Agregar notificaciones por email con reportes diarios autom√°ticos.
2. Implementar modelo de ML para predicci√≥n de salarios.
3. Integrar fuentes adicionales de empleo (alternativas a LinkedIn API).
4. Crear dashboard responsive para m√≥viles.
5. Agregar autenticaci√≥n de usuarios para insights personalizados.
6. Implementar algoritmo de matching entre perfiles y vacantes.

---

## üìû Soporte

- **Issues:** Crear un issue en el repositorio de GitHub.
- **Preguntas:** Canal de Slack del equipo.
- **Documentaci√≥n adicional:** Carpeta `docs/` del repositorio.

---

## üìÑ Licencia

Este proyecto fue desarrollado con fines educativos como parte de una simulaci√≥n laboral en tecnolog√≠a.

---

**Construido con ‚ù§Ô∏è para el Talento Tech de LATAM**

*√öltima actualizaci√≥n: Febrero 2026*