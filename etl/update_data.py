# etl/update_data.py
import os
import pandas as pd
from supabase import create_client
from dotenv import load_dotenv

# Importamos la nueva funci√≥n maestra desde cleaning.py
from cleaning import clean_job_data

load_dotenv()

# ---------------------------------------------------
# CONFIGURACI√ìN SUPABASE
# ---------------------------------------------------
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_SERVICE_KEY") 

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("‚ùå Error: SUPABASE_URL o SUPABASE_SERVICE_KEY no encontrados en .env")

client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ---------------------------------------------------
# üì• CARGA DE DATOS
# ---------------------------------------------------
def load_raw_data():
    """Descarga los datos crudos de las tablas jobs y skills"""
    print("üì• Descargando datos desde Supabase...")
    
    # Descargar Jobs
    jobs_res = client.table("jobs").select("*").execute()
    df_jobs = pd.DataFrame(jobs_res.data)
    
    # Descargar Skills
    skills_res = client.table("skills").select("*").execute()
    df_skills = pd.DataFrame(skills_res.data)

    print(f"üìä Registros recuperados: {len(df_jobs)} jobs y {len(df_skills)} skills.")
    return df_jobs, df_skills

# ---------------------------------------------------
# üîº ACTUALIZACI√ìN (UPSERT)
# ---------------------------------------------------
def upload_data(df_jobs_clean, df_skills_clean):
    """Sube los datos limpios a Supabase usando UPSERT"""
    
    # 1. Actualizar Jobs
    if not df_jobs_clean.empty:
        print(f"‚¨ÜÔ∏è Actualizando {len(df_jobs_clean)} jobs...")
        records = df_jobs_clean.to_dict(orient="records")
        # El on_conflict='job_id' es vital para no duplicar entradas
        client.table("jobs").upsert(records, on_conflict="job_id").execute()
        print("‚úÖ Jobs actualizados correctamente.")

    # 2. Actualizar Skills (vinculadas a los jobs existentes)
    if not df_skills_clean.empty:
        print(f"‚¨ÜÔ∏è Actualizando {len(df_skills_clean)} skills...")
        # Filtrar solo skills cuyos job_id existen en nuestra lista limpia de jobs
        valid_ids = set(df_jobs_clean['job_id'])
        df_skills_filtered = df_skills_clean[df_skills_clean['job_id'].isin(valid_ids)]
        
        records_skills = df_skills_filtered.to_dict(orient="records")
        # Requiere un constraint √∫nico en Supabase para (job_id, skill_name)
        client.table("skills").upsert(records_skills, on_conflict="job_id,skill_name").execute()
        print(f"‚úÖ {len(df_skills_filtered)} skills actualizadas.")
        
# ---------------------------------------------------
# üöÄ LIMPIEZA DE REGISTROS ANTIGUOS
# ---------------------------------------------------
        
def delete_old_jobs(days=30):
    """
    Borra registros m√°s antiguos que N d√≠as para ahorrar espacio en Supabase.
    """
    from datetime import datetime, timedelta
    
    # Calculamos la fecha l√≠mite
    cutoff_date = (datetime.now() - timedelta(days=days)).isoformat()
    
    print(f"üßπ Buscando vacantes publicadas antes de: {cutoff_date}")
    
    try:
        # Filtramos por scraped_at menor a la fecha de corte
        response = client.table("jobs").delete().lt("scraped_at", cutoff_date).execute()
        
        # En la respuesta viene la lista de lo que se borr√≥
        num_deleted = len(response.data) if response.data else 0
        print(f"‚úÖ Se han eliminado {num_deleted} registros antiguos.")
    except Exception as e:
        print(f"‚ùå Error al intentar purgar datos antiguos: {e}")

# ---------------------------------------------------
# üöÄ PROCESO PRINCIPAL (MAIN)
# ---------------------------------------------------
def run_etl():
    delete_old_jobs(days=30)  # Opcional: borrar datos m√°s viejos a 30 d√≠as
    # 1. Cargar
    df_jobs, df_skills = load_raw_data()
    
    if df_jobs.empty:
        print("‚ö†Ô∏è No hay datos en la tabla 'jobs' para procesar.")
        return

    # 2. Limpiar (Usando la l√≥gica de cleaning.py)
    print("\nüßπ Iniciando limpieza de datos...")
    df_jobs_clean = clean_job_data(df_jobs)
    
    # 3. Subir
    upload_data(df_jobs_clean, df_skills)
    
    print("\nüéØ Proceso ETL finalizado con √©xito.")

if __name__ == "__main__":
    run_etl()